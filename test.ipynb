{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Measures: tensor([[-4.3716e+00, -3.1003e+01,  1.7688e+01,  8.7368e+00,  2.4338e+00,\n",
      "          1.6895e+01, -2.7440e+00,  6.0616e+00, -3.9590e+00, -6.3551e+00,\n",
      "          5.8090e+00, -1.4720e-02, -1.3578e+01,  7.0553e+00, -2.6534e+00],\n",
      "        [ 1.1666e+01, -2.4807e+01,  2.5133e+01,  4.1415e-01,  5.6774e-01,\n",
      "          1.5534e+01, -1.6875e+01, -8.1663e+00, -5.3984e+00, -2.6455e+00,\n",
      "          5.5182e+00,  6.6557e+00, -1.5991e+01,  9.9146e+00, -1.5206e+00],\n",
      "        [ 1.6452e+01, -1.7559e+01,  1.7348e+01,  1.1335e+00, -5.3581e+00,\n",
      "          1.2324e+01, -7.2347e+00, -7.7670e+00, -1.0024e+01, -1.2260e+00,\n",
      "          2.3675e+00,  1.7454e+01, -1.4349e+01,  1.0090e+01, -1.3649e+01],\n",
      "        [ 1.1967e+01, -2.1283e+01,  1.0097e+01, -4.3249e+00, -1.0027e+01,\n",
      "          8.6859e+00, -1.1921e+00, -7.6272e+00, -2.3004e+00,  9.2128e-01,\n",
      "          6.4322e+00,  1.2631e+01, -6.6069e-01,  7.4484e+00, -1.0766e+01],\n",
      "        [ 1.2816e+01, -1.6387e+01,  1.4226e+01, -2.7532e+00, -1.6658e+01,\n",
      "          2.0800e+01,  3.1645e+00, -1.2275e+01, -1.7789e+00, -1.1629e+01,\n",
      "         -6.2030e+00,  4.4548e+00,  2.3349e+01, -7.3226e+00, -3.8035e+00],\n",
      "        [ 3.5726e+00, -1.6800e+01,  1.5879e+01, -5.5463e+00, -2.6023e+01,\n",
      "          2.2378e+01,  8.5913e+00, -4.8848e+00, -9.0369e+00, -1.1864e+01,\n",
      "         -7.7933e+00,  4.8162e+00,  2.9901e+01, -4.0313e+00,  8.4168e-01],\n",
      "        [-1.0686e+01, -2.5755e+01,  7.8060e+00, -6.7907e+00, -2.5720e+01,\n",
      "          2.0676e+01,  1.8750e+01,  3.2851e+00,  5.9528e+00, -1.2832e+01,\n",
      "          7.6818e-01,  3.7399e-01,  3.7007e+01, -5.1645e+00, -7.6704e+00],\n",
      "        [-9.5264e+00, -1.7094e+01,  1.8896e+01, -1.1148e+01, -3.0184e+01,\n",
      "          3.1278e+01,  1.2121e+01, -5.6373e+00, -6.2118e-01, -1.8930e+01,\n",
      "          2.4967e-01, -3.2956e+00,  3.8303e+01, -3.1178e+00, -1.2934e+00],\n",
      "        [-4.3775e+00, -5.8582e+00,  1.5205e+01, -1.3264e+01, -3.5877e+01,\n",
      "          3.0087e+01,  1.7212e+01, -7.9653e+00,  3.5094e+00, -2.4777e+01,\n",
      "          6.6526e-01, -1.0781e+01,  4.8023e+01, -6.8298e+00, -4.9710e+00],\n",
      "        [-1.5223e+01, -1.7566e+01,  6.4710e+00, -1.2590e+01, -3.0673e+01,\n",
      "          2.0040e+01,  1.5425e+01,  3.1289e+00,  8.3297e+00, -2.2169e+01,\n",
      "          1.1740e+01, -6.2767e+00,  4.3445e+01, -7.9962e-01, -3.2803e+00],\n",
      "        [-2.8390e+00, -1.1934e+01,  7.9153e+00, -5.0317e+00, -3.2384e+01,\n",
      "          2.5304e+01,  2.1955e+01, -1.3519e+00, -1.7158e+00, -2.5104e+01,\n",
      "          5.3080e+00, -1.2118e+01,  3.9735e+01, -3.8544e+00, -3.8846e+00],\n",
      "        [-4.8682e+00, -6.8767e+00,  5.3558e+00, -4.0322e+00, -2.8603e+01,\n",
      "          2.2288e+01,  1.8377e+01, -2.8418e+00, -1.4006e+01, -1.7239e+01,\n",
      "         -1.0021e-01, -1.4136e+00,  3.1649e+01, -1.3031e+00,  3.6157e+00],\n",
      "        [-9.1328e+00, -2.1177e+01, -4.8596e+00,  2.6499e+00, -2.1674e+01,\n",
      "          7.8276e+00,  2.1049e+01,  1.1677e+01, -1.3819e+01, -7.7324e+00,\n",
      "         -1.0431e+00, -3.2557e+00,  2.8248e+01, -2.1635e+00,  1.3405e+01],\n",
      "        [-2.5168e+01, -2.4454e+01,  5.1429e+00,  4.5252e+00, -1.4570e+01,\n",
      "          9.3295e+00,  1.0776e+01,  2.3379e+01, -1.5106e+01, -7.5732e+00,\n",
      "         -7.3680e+00, -2.4973e+00,  2.2249e+01, -1.2990e+00,  2.2634e+01],\n",
      "        [-2.0035e+01, -2.1011e+01,  2.1152e+01, -3.5043e+00, -2.2953e+01,\n",
      "          1.8614e+01,  1.7617e+01,  1.4434e+01, -1.8913e+01,  6.1761e+00,\n",
      "         -1.0422e+01, -1.6261e+01,  1.1896e+01,  5.3014e+00,  1.7908e+01],\n",
      "        [-2.1005e+01, -1.5462e+01,  1.3614e+01, -4.2648e+00, -1.0909e+01,\n",
      "          7.3722e+00,  3.7677e+00,  1.4001e+01, -1.2859e+01,  6.5882e+00,\n",
      "         -3.4798e+00, -6.4313e+00,  3.1437e+00,  7.9064e+00,  1.8018e+01]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Assume CVs have already been learned as `control_vectors` in an original high-dimensional space\n",
    "class CVMeasureCalculator:\n",
    "    def __init__(self, control_vectors, token_dim=768, cv_dim=15, pca_components=10, sliding_window_size=5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            control_vectors (torch.Tensor): Pre-learned Control Vectors, shape [cv_dim, token_dim]\n",
    "            token_dim (int): Dimensionality of the token embeddings.\n",
    "            cv_dim (int): Number of Control Vectors to use.\n",
    "            pca_components (int): Reduced dimensionality for faster computation via PCA.\n",
    "            sliding_window_size (int): Size of the sliding window.\n",
    "        \"\"\"\n",
    "        self.token_dim = token_dim\n",
    "        self.cv_dim = cv_dim\n",
    "        self.sliding_window_size = sliding_window_size\n",
    "\n",
    "        # Step 1: Dimensionality reduction on control vectors\n",
    "        self.pca = PCA(n_components=pca_components)\n",
    "        self.reduced_cvs = torch.tensor(self.pca.fit_transform(control_vectors.cpu().numpy())).float()\n",
    "\n",
    "    def calculate_cv_measures(self, token_embeddings):\n",
    "        \"\"\"\n",
    "        Calculate CV measures for input tokens.\n",
    "\n",
    "        Parameters:\n",
    "            token_embeddings (torch.Tensor): Token embeddings, shape [num_tokens, token_dim].\n",
    "\n",
    "        Returns:\n",
    "            cv_measures (torch.Tensor): Adversarial measure per token, shape [num_tokens, cv_dim]\n",
    "        \"\"\"\n",
    "        # Step 2: Reduce token embeddings to same dimensionality as reduced CVs\n",
    "        reduced_embeddings = torch.tensor(self.pca.transform(token_embeddings.cpu().numpy())).float()\n",
    "\n",
    "        # Step 3: Calculate measure as the dot product of reduced embeddings and reduced CVs\n",
    "        cv_measures = torch.matmul(reduced_embeddings, self.reduced_cvs.T)\n",
    "\n",
    "        # Step 4: Apply sliding window to aggregate CV measures\n",
    "        cv_measures = self.sliding_window_aggregate(cv_measures)\n",
    "\n",
    "        return cv_measures\n",
    "\n",
    "    def sliding_window_aggregate(self, cv_measures):\n",
    "        \"\"\"\n",
    "        Aggregate CV measures across tokens using a sliding window.\n",
    "\n",
    "        Parameters:\n",
    "            cv_measures (torch.Tensor): Raw CV measures, shape [num_tokens, cv_dim]\n",
    "\n",
    "        Returns:\n",
    "            aggregated_measures (torch.Tensor): Aggregated CV measures, shape [num_tokens, cv_dim]\n",
    "        \"\"\"\n",
    "        num_tokens = cv_measures.size(0)\n",
    "        aggregated_measures = []\n",
    "\n",
    "        for i in range(num_tokens - self.sliding_window_size + 1):\n",
    "            window = cv_measures[i: i + self.sliding_window_size]\n",
    "            aggregated_measures.append(window.mean(dim=0))  # Aggregate within the window\n",
    "\n",
    "        return torch.stack(aggregated_measures)\n",
    "\n",
    "# Initialize example parameters\n",
    "token_dim = 768  # Example token embedding dimension\n",
    "cv_dim = 15  # Number of control vectors\n",
    "num_tokens = 20  # Number of tokens in example input\n",
    "\n",
    "# Generate random embeddings and CVs\n",
    "control_vectors = torch.randn(cv_dim, token_dim)  # Learned Control Vectors\n",
    "token_embeddings = torch.randn(num_tokens, token_dim)  # Token embeddings\n",
    "\n",
    "# Initialize CV measure calculator\n",
    "cv_calculator = CVMeasureCalculator(control_vectors, token_dim=token_dim, cv_dim=cv_dim, pca_components=10, sliding_window_size=5)\n",
    "\n",
    "# Calculate CV measures\n",
    "cv_measures = cv_calculator.calculate_cv_measures(token_embeddings)\n",
    "\n",
    "print(\"CV Measures:\", cv_measures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_location = \"/opt/extra/avijit/projects/rlof/Ryan/zzzzzz/PRISM/cvs/trained_cvs-15.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m direction_signs \u001b[38;5;241m=\u001b[39m pca_reader\u001b[38;5;241m.\u001b[39mdirection_signs  \u001b[38;5;66;03m# signs to adjust directions\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Apply signs to directions and convert to tensor\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m signed_directions \u001b[38;5;241m=\u001b[39m \u001b[43mdirections\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdirection_signs\u001b[49m  \u001b[38;5;66;03m# element-wise multiplication\u001b[39;00m\n\u001b[1;32m     20\u001b[0m cv_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(signed_directions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Store in dictionary with CV name\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def batch_measure_cv_on_texts(cv_name, texts, model, cv_reader):\n",
    "    \"\"\"\n",
    "    Batch computes CV measures on multiple texts using the PCARepReader and the model.\n",
    "    \n",
    "    Args:\n",
    "        cv_name (str): The name of the CV to measure.\n",
    "        texts (list of str): List of text strings to measure the CV on.\n",
    "        model (transformers.PreTrainedModel): The model providing hidden states.\n",
    "        cv_reader (repe.rep_readers.PCARepReader): The CV reader for this CV, providing directions and signs.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of averaged CV scores for each text in the batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine hidden layers to use\n",
    "    hidden_layers = list(range(-1, -model.config.num_hidden_layers, -1))\n",
    "    layer_signs = cv_reader.direction_signs\n",
    "\n",
    "    # Preallocate tensor for batch scores\n",
    "    batch_scores = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Run the text through the model to get hidden states (assuming `get_hidden_states` does this)\n",
    "        hidden_states = model.get_hidden_states(text, layers=hidden_layers)\n",
    "\n",
    "        # Initialize list to store layer-wise CV scores for this text\n",
    "        cv_scores = []\n",
    "        \n",
    "        # Calculate CV score for each layer and scale by direction_signs\n",
    "        for idx, layer in enumerate(hidden_layers):\n",
    "            layer_state = hidden_states[layer][:, -1]  # Get last token's hidden state\n",
    "            layer_score = torch.dot(layer_state, cv_reader.directions[idx]) * layer_signs[idx]\n",
    "            cv_scores.append(layer_score)\n",
    "        \n",
    "        # Calculate the mean CV score across layers for the text\n",
    "        batch_scores.append(torch.mean(torch.stack(cv_scores)))\n",
    "\n",
    "    # Stack the scores into a tensor for the batch\n",
    "    return torch.stack(batch_scores)\n",
    "\n",
    "# Example usage\n",
    "# Replace 'model' with your actual model instance\n",
    "# Replace 'cv_dict' with your loaded CV dictionary\n",
    "model = \"/opt/extra/avijit/projects/rlof/Meta-Llama-3.1-8B-Instruct\"\n",
    "texts = [\"Example text 1\", \"Example text 2\"]\n",
    "cv_scores = batch_measure_cv_on_texts(\"honesty\", texts, model, cv_dict[\"honesty\"])\n",
    "\n",
    "print(f\"CV Scores for batch: {cv_scores}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
